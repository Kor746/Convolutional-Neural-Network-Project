{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tce3stUlHN0L"
   },
   "source": [
    "# Deep Learning Assignment 3\n",
    "\n",
    "#### Professor: Francois Rivest\n",
    "#### Name: Daniel Lee (20125992)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Convolutional Neural Network using Estimators\n",
    "\n",
    "Explanation: \n",
    "\n",
    "1. LINE 1: The imports are required packages to build the CNN. \n",
    "\n",
    "2. LINE 6: tf.logging.set_verbosity sets the threshold for the type of log message that will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6-tpguHLP6Rm"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "1. LINE 4: The input layer is created with the MNIST dataset, which is reshaped into a tensor with the dimensionss: -1 (batch size) x 28 (height) x 28 (width) x 1 (channel). The -1 places each 28 x 28 image into a column vector as a row. The 1 represents the number of channel layers used, which is just 1 layer in this case because we only need the colour black for the MNIST dataset.\n",
    "\n",
    "2. LINE 7: The first convolutional layer applies 32 filters that are of 5 x 5 kernel size. The filter contains randomly initialized weights, which is multiplied on each sub-region of an image. In addition, the parameter, padding=\"same\" is used to pad the sub-regions with a low value (e.g. 0) where the filter exceeds the right or bottom boundary of an image. The following process is done 32 times (each filter) for each image in the batch: An overlapping filter of 5 x 5 kernel size is applied on each subregion of an image, which produces a single output value. The ReLu activation function is then applied to the output of each sub-region to add non-linearity, then is stored in a new tensor (i.e., feature map). Thus, a tensor of batch size x 28 x 28 x 32 dimension is created.\n",
    "\n",
    "3. LINE 15: Max pooling is then used to reduce the dimensionality or downsample the non-linearities of the 28 x 28 feature maps from convolutional layer 1. The following process is done 32 times (each filter) for each image in the batch: A non-overlapping filter of 2 x 2 kernel size with a stride of 2 is applied on each sub-region of the feature map, so that the filter only extracts the maximum value in each sub-region, which are placed in a new tensor. Thus, this creates a tensor of batch size x 14 x 14 x 32 dimension.\n",
    "\n",
    "4. LINE 18: The second convolutional layer applies 64 filters that are of 5 x 5 kernel size. The following process is done 64 times (each filter) for each image in the batch: An overlapping 5 x 5 kernel size is applied on each of sub-region of the tensor, which produces a single output value. The ReLu activation function is then applied to the output of each sub-region to add non-linearity, then is stored in a new tensor (i.e., feature map). Thus, a tensor of batch size x 14 x 14 x 64 dimension is created.\n",
    "\n",
    "5. LINE 24: Afterwards, max pooling is used again with a filter of 2 x 2 kernel size and a stride of 2 to reduce the dimensionality of the 14 x 14 feature maps. The following process is done 64 times (each filter) for each image in the batch: A non-overlapping filter of 2 x 2 kernel size with a stride of 2 is applied on each sub-region of the feature map, so that the filter only extracts the maximum value in each sub-region, which are placed in a new tensor. Thus, this creates a tensor of batch size x 7 x 7 x 64 dimension.\n",
    "\n",
    "6. LINE 27: The tensor from the second pooling layer is flattened into a 1-dimensional tensor where the length is now batch size x 7 x 7 x 64 because the dense or fully connected layer takes a 1-dimensional tensor as input.\n",
    "\n",
    "7. LINE 28: The dense layer uses 1,024 nodes (i.e neurons), where each node is densely connected to each node in the dropout layer. Each node will use the 1-dimensional tensor of length batch size x 7 x 7 x 64, and then apply the ReLu activation function, which then produces a tensor of batch size x 1,024 dimension.  \n",
    "\n",
    "8. LINE 29: A dropout layer (i.e., a form of regularization) is used to prevent overfitting when training by shutting off certain nodes based on the given probability. Subsequent nodes are able to find patterns from different parts of the data by looking at fewer samples. In this code block, the 0.4 represents the probability of dropping or shutting down a node in the layer. Dropout is applied on each node of the dense layer.\n",
    "\n",
    "9. LINE 33: The outputs from the dropout layer then become the inputs for the nodes of the logit layer. In our case, the logit layer has 10 nodes or neurons to match the number of target classes, which in our case are the digits from 0 to 9. The logit function is applied on each output from the dropout layer (batch size x 1,024 tensor, and then applies a linear activation function to produce raw values in a batch size x 10 tensor.\n",
    "\n",
    "10. LINE 35: Argmax is then used to get the index of the element with the highest value in each row of the batch size x 10 tensor that was output from the logit layer. The softmax activation function then creates a probability distribution for the 10 values (sums up to 1) from the argmax function for each image in the batch. The probability distribution in the batch size x 10 tensor represents the predictions for each target class for each image.\n",
    "\n",
    "11. LINE 43: IF the mode of the model or estimator is being used to predict. If so, an EstimatorSpec model is returned with the mode and prediction values as arguments.\n",
    "\n",
    "12. LINE 47: The softmax cross entropy loss function is used to calculate the loss between the target labels and the logit values. \n",
    "\n",
    "13. LINE 50: IF the mode of the model or estimator is being used to train then it uses a stochastic gradient descent optimizer to shuffle the samples with an alpha or learning rate parameter of 0.001, which optimally locates the the minimum on the cross entropy loss function. The optimizer then uses the minimize function to compute the gradients of the loss with respect to the variable (e.g., weights) that were stored on the tensor graph. Then it uses the computed gradients to update the variables such as the weights in the tensor graph. Afterwards, the EstimatorSpec model is returned with the mode, loss, and the training step from the minimize function as arguments.\n",
    "\n",
    "14. LINE 58: IF the mode of the model or estimator is being used to evaluate then the model computes the accuracy of the class predictions with respect to the actual labels.\n",
    "\n",
    "15. LINE 62: This returns the complete EstimatorSpec model that can be used, with an explicit mode (e.g., training, predicting, evaluating), the loss, and the evaluation metrics as arguments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gMR-_3rkRKPa"
   },
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "  \"\"\"Model function for CNN.\"\"\"\n",
    "  # Input Layer\n",
    "  input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n",
    "\n",
    "  # Convolutional Layer #1\n",
    "  conv1 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=32,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "  # Pooling Layer #1\n",
    "  pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "  # Convolutional Layer #2 and Pooling Layer #2\n",
    "  conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=64,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "  pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "  # Dense Layer\n",
    "  pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "  dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "  dropout = tf.layers.dropout(\n",
    "      inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "  # Logits Layer\n",
    "  logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "\n",
    "  predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "  }\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "  # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "  # Configure the Training Op (for TRAIN mode)\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    train_op = optimizer.minimize(\n",
    "        loss=loss,\n",
    "        global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "  # Add evaluation metrics (for EVAL mode)\n",
    "  eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions[\"classes\"])\n",
    "  }\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y2Bwe-AdTRzX"
   },
   "source": [
    "## Training and Evaluating the CNN MNIST Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6EC9aOY2TTLU"
   },
   "source": [
    "Explanation:\n",
    "\n",
    "1. LINE 2: The MNIST dataset is loaded and separated into training data and training label variables, along with test data and test label variables. \n",
    "\n",
    "2. LINE 5: The training data is normalized by 255 (RGB). \n",
    "\n",
    "3. LINE 6: Casts the training labels to a 32-bit integer type.\n",
    "\n",
    "4. LINE 8: The training data is normalized by 255 (RGB). \n",
    "\n",
    "5. LINE 9: Casts the training labels to a 32-bit integer type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ccobb0qETV-S"
   },
   "outputs": [],
   "source": [
    "# Load training and eval data\n",
    "((train_data, train_labels),\n",
    " (eval_data, eval_labels)) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_data = train_data/np.float32(255)\n",
    "train_labels = train_labels.astype(np.int32)  # not required\n",
    "\n",
    "eval_data = eval_data/np.float32(255)\n",
    "eval_labels = eval_labels.astype(np.int32)  # not required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S2_Isc7kTa45"
   },
   "source": [
    "Explanation:\n",
    "\n",
    "1. LINE 2: Creates the classifier or model using the cnn_model_fn function from above, and the file path of where the model should be saved as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yjC6HdwZTdg4"
   },
   "outputs": [],
   "source": [
    "# Create the Estimator\n",
    "mnist_classifier = tf.estimator.Estimator(\n",
    "    model_fn=cnn_model_fn, model_dir=\"/tmp/mnist_convnet_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_6ow7hVYTm3f"
   },
   "source": [
    "Explanation:\n",
    "\n",
    "1. LINE 2: Creates a mapping of the probabilities with the tensor from the softmax layer that it associates with.\n",
    "\n",
    "2. LINE 4: Sets up a hook in the CNN to log a dictionary that contains a corresponding label to a tensor from the TensorFlow graph for every 50 steps in an epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S6T10kssTpdz"
   },
   "outputs": [],
   "source": [
    "# Set up logging for predictions\n",
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "\n",
    "logging_hook = tf.train.LoggingTensorHook(\n",
    "    tensors=tensors_to_log, every_n_iter=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "brVs1dRMT0NM"
   },
   "source": [
    "Explanation:\n",
    "\n",
    "1. LINE 2: This creates an input object with parameters that use mini-batches of a 100 samples from the training data and labels, which are randomly shuffled, and uses the step count to signify when the training ends instead of epochs (none).\n",
    "\n",
    "2. LINE 10: This trains the model with the input object defined above with only 1 step count, and the logging hook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h-dewpleT2sk"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": train_data},\n",
    "    y=train_labels,\n",
    "    batch_size=100,\n",
    "    num_epochs=None,\n",
    "    shuffle=True)\n",
    "\n",
    "# train one step and display the probabilties\n",
    "mnist_classifier.train(\n",
    "    input_fn=train_input_fn,\n",
    "    steps=1,\n",
    "    hooks=[logging_hook])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gyNSE3e-14Lq"
   },
   "source": [
    "Explanation:\n",
    "\n",
    "1. LINE 1: This uses the mini-batches to train the model with 1,000 steps per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cri6zqcf2IXY"
   },
   "outputs": [],
   "source": [
    "mnist_classifier.train(input_fn=train_input_fn, steps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4bQdkLMeUE5U"
   },
   "source": [
    "Explanation:\n",
    "\n",
    "1. LINE 1: After training the model, this function computes the accuracy of the test set and the test labels on the newly trained model. Only 1 epoch is needed, since it is just a forward pass to get the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I0RGiqd0UF0N"
   },
   "outputs": [],
   "source": [
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": eval_data},\n",
    "    y=eval_labels,\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "\n",
    "eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)\n",
    "print(eval_results)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Tce3stUlHN0L"
   ],
   "name": "cnn.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
